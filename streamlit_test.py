from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.llms import OpenAI
import pandas as pd
import streamlit as st
import time
from tenacity import retry, stop_after_attempt, wait_random_exponential

# Streamlit ìƒíƒœ ì´ˆê¸°í™”
if "openai_model" not in st.session_state:
    st.session_state["openai_model"] = "gpt-3.5-turbo"

if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", "content": "How can I help you?"}]

# CSV íŒŒì¼ ê²½ë¡œ ì„¤ì •
csv_file_path = 'ìŒì‹ì .csv'  # CSV íŒŒì¼ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”

# ë°ì´í„° ë¡œë“œ ë° ì¤€ë¹„
try:
    data = pd.read_csv(csv_file_path, encoding='cp949')
except Exception as e:
    st.error(f"CSV íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    st.stop()

# í•„ìš”í•œ ì¹¼ëŸ¼ í™•ì¸
if 'all_about' not in data.columns or 'ìƒí˜¸ëª…' not in data.columns:
    st.error("CSV íŒŒì¼ì— 'all_about' ë˜ëŠ” 'ìƒí˜¸ëª…' ì¹¼ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    st.stop()

# í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (í•„ìš” ì‹œ ì¡°ì •)
MAX_TEXT_LENGTH = 1000
data['all_about'] = data['all_about'].apply(lambda x: x[:MAX_TEXT_LENGTH] if len(x) > MAX_TEXT_LENGTH else x)

# ì„ë² ë”© ì´ˆê¸°í™”
embeddings = OpenAIEmbeddings()

# FAISS Vectorstore ìƒì„± í•¨ìˆ˜ (ë°°ì¹˜ ì²˜ë¦¬ ë° ì˜¤ë¥˜ í•¸ë“¤ë§ í¬í•¨)
@retry(stop=stop_after_attempt(3), wait=wait_random_exponential(min=1, max=60))
def create_faiss_vectorstore(texts, embeddings):
    all_vectors = []
    batch_size = 10  # ë°°ì¹˜ í¬ê¸° ì„¤ì •

    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i + batch_size]
        try:
            batch_vectors = embeddings.embed_documents(batch_texts)
            all_vectors.extend(batch_vectors)
            time.sleep(1)  # API í˜¸ì¶œ ê°„ ëŒ€ê¸° ì‹œê°„ ì¶”ê°€
        except Exception as e:
            st.warning(f"ì„ë² ë”© ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ë°°ì¹˜ {i // batch_size + 1}): {str(e)}")
            raise e

    return FAISS.from_embeddings(all_vectors, embeddings)

# ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
try:
    vectorstore = create_faiss_vectorstore(data['all_about'].tolist(), embeddings)
except Exception as e:
    st.error(f"ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    st.stop()

# Streamlit UI ì„¤ì •
st.title("ğŸ’¬ Chatbot")
st.caption("ğŸš€ A Streamlit chatbot powered by OpenAI")

for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

if prompt := st.chat_input():
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)

    # ì…ë ¥ í…ìŠ¤íŠ¸ ì„¤ì •
    input_text = prompt

    # ì…ë ¥ í…ìŠ¤íŠ¸ì™€ ê°€ì¥ ìœ ì‚¬í•œ 5ê°œ í•­ëª© ê²€ìƒ‰
    try:
        similarities = vectorstore.similarity_search(input_text, k=5)
        similar_names = [data.loc[data['all_about'] == match.page_content, 'ìƒí˜¸ëª…'].values[0] for match in similarities]
    except Exception as e:
        st.error(f"ìœ ì‚¬ í•­ëª© ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        st.stop()

    # ChatGPT ì´ˆê¸°í™”
    llm = OpenAI(temperature=0.7)

    # ChatGPTë¡œ ê°„ë‹¨í•œ ì„¤ëª… ìƒì„±
    explanations = []
    for name in similar_names:
        try:
            response = llm(
                f"""'{name}'ì™€ ê´€ë ¨ëœ ë‚´ìš©ì„ ê°„ë‹¨íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”. ë‹¹ì‹ ì€ ëŒ€êµ¬ê´‘ì—­ì‹œ ë§›ì§‘ì¶”ì²œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."""
            )
            explanations.append(response)
        except Exception as e:
            explanations.append(f"'{name}'ì— ëŒ€í•œ ì„¤ëª… ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

    # ê²°ê³¼ ìƒì„± ë° ì¶œë ¥
    result = "\n".join([f"{idx}. {name} : {explanation}" for idx, (name, explanation) in enumerate(zip(similar_names, explanations), start=1)])

    st.session_state.messages.append({"role": "assistant", "content": result})
    st.chat_message("assistant").write(result)
